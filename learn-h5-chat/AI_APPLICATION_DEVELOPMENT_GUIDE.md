# ä»å¼€å‘è€…åˆ°AIåº”ç”¨å¼€å‘è€… - å®Œæ•´å­¦ä¹ æŒ‡å—

> æœ¬æŒ‡å—ä¸ºå·²æŒæ¡Reactçš„å¼€å‘è€…è®¾è®¡ï¼Œå¸®åŠ©ä½ å¿«é€Ÿå­¦ä¹ AIåº”ç”¨å¼€å‘çš„æ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚

## ğŸ“š ç›®å½•
1. [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
2. [APIæ¥å…¥æ¨¡å¼](#apiæ¥å…¥æ¨¡å¼)
3. [é€šç”¨åŒ–AIæ¨¡å‹é›†æˆ](#é€šç”¨åŒ–aiæ¨¡å‹é›†æˆ)
4. [æµå¼è¾“å‡ºå®ç°](#æµå¼è¾“å‡ºå®ç°)
5. [ç°ä»£AIæ¨¡å‹ç”Ÿæ€](#ç°ä»£aiæ¨¡å‹ç”Ÿæ€)
6. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)
7. [å®Œæ•´ä»£ç ç¤ºä¾‹](#å®Œæ•´ä»£ç ç¤ºä¾‹)

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1. AI APIè°ƒç”¨çš„æœ¬è´¨
æ‰€æœ‰AI APIæœ¬è´¨ä¸Šéƒ½æ˜¯HTTPè¯·æ±‚ï¼š
```
ç”¨æˆ·è¾“å…¥ â†’ æ„å»ºè¯·æ±‚ â†’ å‘é€åˆ°API â†’ è·å–å“åº” â†’ å±•ç¤ºç»“æœ
```

### 2. ä¸‰å¤§AI APIæä¾›å•†å¯¹æ¯”

| ç‰¹æ€§ | Google Gemini | OpenAI | å¼€æº/è‡ªæ‰˜ç®¡ |
|------|---------------|--------|-----------|
| æ¥å£åè®® | Google GenAI SDK / REST | OpenAI SDK / REST | OpenAIå…¼å®¹æ ¼å¼ |
| ä¸»è¦æ¨¡å‹ | Gemini 2.5ç³»åˆ— | GPT-4ç³»åˆ— | Llamaã€Mistralç­‰ |
| ç‰¹æ®ŠåŠŸèƒ½ | æ€è€ƒé¢„ç®—(Think) | Vision API | æœ¬åœ°éƒ¨ç½² |
| æµå¼è¾“å‡º | âœ… æ”¯æŒ | âœ… æ”¯æŒ | âœ… æ”¯æŒ |
| å¯åŠ¨æˆæœ¬ | ä½(å…è´¹å±‚) | æŒ‰é‡ä»˜è´¹ | 0(è‡ªæ‰˜ç®¡) |

### 3. å…³é”®æœ¯è¯­

- **æ¨¡å‹(Model)**: AIå¤„ç†å¼•æ“çš„æ ‡è¯†ç¬¦ï¼Œå¦‚ `gpt-4-turbo`ã€`gemini-2.5-pro`
- **APIå¯†é’¥(API Key)**: è®¤è¯å‡­è¯ï¼Œç”¨äºæ ‡è¯†è°ƒç”¨è€…
- **æ¸©åº¦(Temperature)**: 0-2ï¼Œæ§åˆ¶å“åº”çš„åˆ›æ„åº¦ï¼ˆä½=ç¡®å®šï¼Œé«˜=åˆ›æ„ï¼‰
- **Token**: æ–‡æœ¬çš„æœ€å°å¤„ç†å•ä½ï¼Œ1ä¸ªtokenâ‰ˆ4ä¸ªè‹±æ–‡å­—ç¬¦æˆ–1ä¸ªä¸­æ–‡å­—
- **æµå¼å“åº”(Streaming)**: é€å­—è¿”å›ç»“æœï¼Œè€Œéç­‰å¾…å®Œæ•´å“åº”

---

## APIæ¥å…¥æ¨¡å¼

### æ¨¡å¼1: ç›´æ¥REST APIè°ƒç”¨ï¼ˆæ¨èå­¦ä¹ ï¼‰

æ‰€æœ‰ç°ä»£AI APIéƒ½æ”¯æŒHTTP RESTè°ƒç”¨ï¼Œè¿™æ˜¯æœ€åŸºç¡€çš„é›†æˆæ–¹å¼ã€‚

```typescript
// æ ¸å¿ƒæ¨¡å¼ï¼šå‘é€è¯·æ±‚ â†’ å¤„ç†å“åº”
async function callAI(userMessage: string) {
  const response = await fetch('https://api.provider.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${API_KEY}`
    },
    body: JSON.stringify({
      model: 'gpt-4-turbo',
      messages: [
        { role: 'system', content: 'ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹' },
        { role: 'user', content: userMessage }
      ],
      temperature: 0.7,
      max_tokens: 1000
    })
  })
  
  const data = await response.json()
  return data.choices[0].message.content
}
```

### æ¨¡å¼2: OpenAIå…¼å®¹APIï¼ˆæœ¬åœ°+äº‘æœåŠ¡é€šç”¨ï¼‰

è¿™æ˜¯ç°åœ¨çš„å·¥ä¸šæ ‡å‡†ã€‚æ‰€æœ‰å…¼å®¹çš„APIä½¿ç”¨**ç›¸åŒçš„è¯·æ±‚æ ¼å¼**ï¼š

```typescript
// é€‚ç”¨äºï¼šOpenAIã€Ollamaã€LM Studioã€Mistralã€Togetherç­‰
const providers = {
  'openai': 'https://api.openai.com/v1',
  'ollama': 'http://localhost:11434/v1',
  'lm-studio': 'http://localhost:1234/v1',
  'mistral': 'https://api.mistral.ai/v1',
  'together': 'https://api.together.xyz/v1'
}

async function callOpenAICompatibleAPI(
  message: string,
  baseUrl: string,
  modelId: string,
  apiKey?: string
) {
  const response = await fetch(`${baseUrl}/chat/completions`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      ...(apiKey && { 'Authorization': `Bearer ${apiKey}` })
    },
    body: JSON.stringify({
      model: modelId,
      messages: [{ role: 'user', content: message }]
    })
  })
  
  return (await response.json()).choices[0].message.content
}
```

### æ¨¡å¼3: å®˜æ–¹SDKï¼ˆç”Ÿäº§ç¯å¢ƒé¦–é€‰ï¼‰

æ¯ä¸ªæä¾›å•†éƒ½æä¾›SDKï¼Œç”¨äºç±»å‹å®‰å…¨å’Œé”™è¯¯å¤„ç†ï¼š

```typescript
// Google Gemini SDK
import { GoogleGenAI } from '@google/genai'

const genAI = new GoogleGenAI({ apiKey: process.env.GEMINI_API_KEY })
const model = genAI.models.getModel('gemini-2.5-pro')
const response = await model.generateContent({ contents: 'Your message' })

// OpenAI SDK
import OpenAI from 'openai'

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY })
const response = await client.chat.completions.create({
  model: 'gpt-4-turbo',
  messages: [{ role: 'user', content: 'Your message' }]
})
```

---

## é€šç”¨åŒ–AIæ¨¡å‹é›†æˆ

### å…³é”®é—®é¢˜ï¼šå¦‚ä½•å¤„ç†ä¸åŒæ¨¡å‹çš„å·®å¼‚ï¼Ÿ

ä¸åŒæ¨¡å‹çš„APIæœ‰ç»†å¾®å·®å¼‚ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ª**é€‚é…å±‚**æ¥ç»Ÿä¸€å®ƒä»¬ã€‚

### è§£å†³æ–¹æ¡ˆï¼šå·¥å‚æ¨¡å¼ + é€‚é…å™¨

```typescript
// 1. å®šä¹‰é€šç”¨æ¥å£
interface AIProvider {
  callAI(params: CallParams): Promise<AIResponse>
}

interface CallParams {
  message: string
  systemPrompt?: string
  temperature?: number
  maxTokens?: number
}

interface AIResponse {
  text: string
  durationMs: number
  error?: string
}

// 2. ä¸ºæ¯ä¸ªæä¾›å•†å®ç°é€‚é…å™¨
class OpenAIAdapter implements AIProvider {
  constructor(private apiKey: string, private baseUrl: string) {}
  
  async callAI(params: CallParams): Promise<AIResponse> {
    const startTime = performance.now()
    try {
      const response = await fetch(`${this.baseUrl}/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': `Bearer ${this.apiKey}`
        },
        body: JSON.stringify({
          model: this.modelId,
          messages: [
            ...(params.systemPrompt ? [{ role: 'system', content: params.systemPrompt }] : []),
            { role: 'user', content: params.message }
          ],
          temperature: params.temperature ?? 0.7,
          max_tokens: params.maxTokens ?? 1000
        })
      })
      
      const data = await response.json()
      return {
        text: data.choices[0].message.content,
        durationMs: performance.now() - startTime
      }
    } catch (error) {
      return {
        text: `é”™è¯¯: ${error.message}`,
        durationMs: performance.now() - startTime,
        error: error.name
      }
    }
  }
}

class GeminiAdapter implements AIProvider {
  constructor(private apiKey: string) {}
  
  async callAI(params: CallParams): Promise<AIResponse> {
    const startTime = performance.now()
    try {
      const genAI = new GoogleGenAI({ apiKey: this.apiKey })
      const model = genAI.models.getModel('gemini-2.5-pro')
      
      const response = await model.generateContent({
        contents: params.message,
        systemInstruction: params.systemPrompt,
        config: {
          temperature: params.temperature,
          maxOutputTokens: params.maxTokens
        }
      })
      
      return {
        text: response.text,
        durationMs: performance.now() - startTime
      }
    } catch (error) {
      return {
        text: `é”™è¯¯: ${error.message}`,
        durationMs: performance.now() - startTime,
        error: error.name
      }
    }
  }
}

// 3. å·¥å‚å‡½æ•°
function createAIProvider(config: {
  provider: 'openai' | 'gemini'
  apiKey: string
  baseUrl?: string
  modelId?: string
}): AIProvider {
  switch (config.provider) {
    case 'openai':
      return new OpenAIAdapter(config.apiKey, config.baseUrl || '', config.modelId || '')
    case 'gemini':
      return new GeminiAdapter(config.apiKey)
    default:
      throw new Error('ä¸æ”¯æŒçš„æä¾›å•†')
  }
}

// 4. ä½¿ç”¨
const provider = createAIProvider({
  provider: 'openai',
  apiKey: process.env.OPENAI_API_KEY,
  baseUrl: 'https://api.openai.com/v1'
})

const response = await provider.callAI({
  message: 'ä½ å¥½',
  systemPrompt: 'ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹'
})
```

### æ¨¡å‹èƒ½åŠ›æ£€æµ‹

```typescript
interface ModelCapabilities {
  supportsVision: boolean
  supportsStreaming: boolean
  supportsSystemPrompt: boolean
  maxTokens: number
  costPer1MTokens: {
    input: number
    output: number
  }
}

const modelCapabilities: Record<string, ModelCapabilities> = {
  'gpt-4-turbo': {
    supportsVision: true,
    supportsStreaming: true,
    supportsSystemPrompt: true,
    maxTokens: 128000,
    costPer1MTokens: { input: 10, output: 30 }
  },
  'gemini-2.5-pro': {
    supportsVision: true,
    supportsStreaming: true,
    supportsSystemPrompt: true,
    maxTokens: 1000000,
    costPer1MTokens: { input: 1.25, output: 2.5 }
  },
  'llama3': {
    supportsVision: false,
    supportsStreaming: true,
    supportsSystemPrompt: true,
    maxTokens: 8192,
    costPer1MTokens: { input: 0, output: 0 }
  }
}

function canUseFeature(modelId: string, feature: 'vision' | 'streaming'): boolean {
  const caps = modelCapabilities[modelId]
  if (feature === 'vision') return caps?.supportsVision ?? false
  if (feature === 'streaming') return caps?.supportsStreaming ?? false
  return false
}
```

---

## æµå¼è¾“å‡ºå®ç°

### ä¸ºä»€ä¹ˆéœ€è¦æµå¼è¾“å‡ºï¼Ÿ

- **ç”¨æˆ·ä½“éªŒ**: å®æ—¶çœ‹åˆ°AIåœ¨"æ€è€ƒ"è€Œä¸æ˜¯æ­»ç­‰
- **ç”¨äºé•¿æ–‡æœ¬**: 1000+tokençš„å“åº”æ— éœ€ç­‰å¾…å…¨éƒ¨å®Œæˆ
- **æˆæœ¬ä¼˜åŒ–**: å¯ä»¥æå‰åœæ­¢æµå¼å“åº”

### æµå¼APIåŸºç¡€

æ‰€æœ‰æ”¯æŒæµå¼çš„APIéƒ½ä½¿ç”¨Server-Sent Events (SSE) æ ¼å¼ï¼š

```
data: {"choices":[{"delta":{"content":"ä½ "}}]}
data: {"choices":[{"delta":{"content":"å¥½"}}]}
data: {"choices":[{"delta":{"content":"å—"}}]}
data: [DONE]
```

### å®ç°æµå¼è°ƒç”¨

```typescript
// é€šç”¨æµå¼è°ƒç”¨å‡½æ•°
async function* streamAI(
  message: string,
  provider: 'openai' | 'gemini',
  config: { apiKey: string; baseUrl: string; model: string }
): AsyncGenerator<string> {
  const response = await fetch(`${config.baseUrl}/chat/completions`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`
    },
    body: JSON.stringify({
      model: config.model,
      messages: [{ role: 'user', content: message }],
      stream: true // å…³é”®ï¼šå¯ç”¨æµå¼æ¨¡å¼
    })
  })

  if (!response.ok) {
    throw new Error(`APIé”™è¯¯: ${response.statusText}`)
  }

  const reader = response.body.getReader()
  const decoder = new TextDecoder()

  try {
    while (true) {
      const { done, value } = await reader.read()
      if (done) break

      const chunk = decoder.decode(value)
      const lines = chunk.split('\n')

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6)
          if (data === '[DONE]') break

          try {
            const parsed = JSON.parse(data)
            const content = parsed.choices?.[0]?.delta?.content
            if (content) {
              yield content
            }
          } catch (e) {
            // å¿½ç•¥è§£æé”™è¯¯
          }
        }
      }
    }
  } finally {
    reader.releaseLock()
  }
}

// åœ¨Reactä¸­ä½¿ç”¨
function ChatComponent() {
  const [text, setText] = useState('')
  
  async function handleStream() {
    setText('') // æ¸…ç©ºä¹‹å‰çš„å†…å®¹
    
    for await (const chunk of streamAI('ä½ å¥½', 'openai', {
      apiKey: process.env.OPENAI_API_KEY,
      baseUrl: 'https://api.openai.com/v1',
      model: 'gpt-4-turbo'
    })) {
      setText(prev => prev + chunk)
    }
  }
  
  return (
    <>
      <button onClick={handleStream}>å‘é€</button>
      <div>{text}</div>
    </>
  )
}
```

### æµå¼è¾“å‡º + TypeScriptç±»å‹å®‰å…¨

```typescript
interface StreamOptions {
  onChunk?: (chunk: string) => void
  onError?: (error: Error) => void
  onComplete?: () => void
  signal?: AbortSignal // æ”¯æŒä¸­æ–­
}

async function streamAIWithCallbacks(
  message: string,
  config: AIConfig,
  options: StreamOptions
): Promise<string> {
  const chunks: string[] = []
  
  try {
    for await (const chunk of streamAI(message, config)) {
      chunks.push(chunk)
      options.onChunk?.(chunk)
      
      // æ”¯æŒä¸­æ–­
      if (options.signal?.aborted) {
        throw new Error('æµå¼è¾“å‡ºå·²ä¸­æ–­')
      }
    }
    
    options.onComplete?.()
    return chunks.join('')
  } catch (error) {
    options.onError?.(error as Error)
    throw error
  }
}
```

---

## ç°ä»£AIæ¨¡å‹ç”Ÿæ€

### 2024-2025 æœ€æ–°æ¨¡å‹é€ŸæŸ¥è¡¨

#### é¡¶çº§æ¨ç†æ¨¡å‹ï¼ˆé«˜æ™ºåŠ›ã€é«˜æˆæœ¬ï¼‰

| æ¨¡å‹ | æä¾›å•† | å‘å¸ƒ | ç‰¹ç‚¹ | APIè´¹ç”¨ |
|------|--------|------|------|---------|
| **GPT-4o** | OpenAI | 2024/11 | å¤šæ¨¡æ€ã€å¿«é€Ÿã€å‡†ç¡® | $5/$15 per 1M tokens |
| **Gemini 2.5 Pro** | Google | 2025/01 | æ€è€ƒé¢„ç®—ã€è¶…é•¿ä¸Šä¸‹æ–‡ | $1.25/$2.5 per 1M tokens |
| **Claude 3.5 Sonnet** | Anthropic | 2024/10 | æ¨ç†å¼ºã€å®‰å…¨æ€§å¥½ | $3/$15 per 1M tokens |
| **Llama 3.1 405B** | Meta | 2024/07 | å¼€æºã€è‡ªæ‰˜ç®¡ã€å…è´¹ | è‡ªæ‰˜ç®¡æˆæœ¬ |

#### å¹³è¡¡æ¨¡å‹ï¼ˆæ€§ä»·æ¯”æœ€ä¼˜ï¼‰

| æ¨¡å‹ | æä¾›å•† | ç‰¹ç‚¹ |
|------|--------|------|
| **GPT-4o mini** | OpenAI | è½»é‡çº§ã€æˆæœ¬ä½ |
| **Gemini 2.5 Flash** | Google | è¶…å¿«é€Ÿã€ä½æˆæœ¬ |
| **Claude 3 Haiku** | Anthropic | ç²¾ç®€ç‰ˆClaude |
| **Mistral 7B/8x7B** | Mistral | å¼€æºã€é«˜æ•ˆ |

#### ç‰¹åŒ–æ¨¡å‹

| ç”¨é€” | æ¨èæ¨¡å‹ | ç‰¹ç‚¹ |
|------|---------|------|
| **é•¿æ–‡æ¡£å¤„ç†** | Gemini 2.5 Pro (2M tokens) | ç™¾ä¸‡tokenä¸Šä¸‹æ–‡ |
| **ä»£ç ç”Ÿæˆ** | GPT-4 Turbo / Claude | ä»£ç èƒ½åŠ›å¼º |
| **æœ¬åœ°éƒ¨ç½²** | Llama 3.1 / Mistral | å¼€æºã€è‡ªæ‰˜ç®¡ |
| **å®æ—¶åº”ç”¨** | GPT-4o mini / Gemini Flash | å¿«é€Ÿã€ä½æˆæœ¬ |
| **å¤šæ¨¡æ€** | GPT-4o / Gemini 2.5 Pro | å›¾åƒã€éŸ³é¢‘å¤„ç† |

### æ¨¡å‹å‚æ•°é…ç½®

```typescript
interface ModelConfig {
  modelId: string
  temperature?: number    // 0-2, é»˜è®¤0.7
  topP?: number          // 0-1, é»˜è®¤1
  topK?: number          // 1-40, é»˜è®¤40
  maxTokens?: number
  frequencyPenalty?: number // OpenAI
  presencePenalty?: number  // OpenAI
  thinkingBudget?: number   // Geminiä¸“ç”¨
}

// é’ˆå¯¹ä¸åŒåœºæ™¯çš„å‚æ•°é¢„è®¾
const paramPresets = {
  // ç²¾ç¡®ã€äº‹å®æ€§å›ç­”
  precise: {
    temperature: 0.1,
    topP: 0.9,
    maxTokens: 1000
  },
  // å¹³è¡¡çš„åˆ›æ„å›ç­”
  balanced: {
    temperature: 0.7,
    topP: 0.9,
    maxTokens: 1500
  },
  // é«˜åˆ›æ„å›ç­”
  creative: {
    temperature: 1.5,
    topP: 0.95,
    maxTokens: 2000
  },
  // æ·±åº¦æ€è€ƒï¼ˆä»…Geminiï¼‰
  deepThinking: {
    temperature: 0.5,
    thinkingBudget: 24576,
    maxTokens: 4000
  }
}
```

### å¦‚ä½•é€‰æ‹©æ¨¡å‹ï¼Ÿ

```typescript
function selectBestModel(useCase: string): string {
  const selection: Record<string, string> = {
    'simple-qa': 'gpt-4o-mini',           // æˆæœ¬æœ€ä½
    'complex-reasoning': 'gemini-2.5-pro', // æ€è€ƒèƒ½åŠ›å¼º
    'code-generation': 'gpt-4-turbo',     // ä»£ç æœ€ä¼˜
    'long-document': 'gemini-2.5-pro',    // ç™¾ä¸‡tokenä¸Šä¸‹æ–‡
    'cost-sensitive': 'gemini-2.5-flash', // æ€§ä»·æ¯”æœ€ä¼˜
    'local-deployment': 'llama3.1',        // æœ¬åœ°å…è´¹
    'real-time-app': 'gpt-4o-mini',       // æœ€å¿«
  }
  
  return selection[useCase] || 'gpt-4o-mini'
}
```

---

## æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†ä¸é‡è¯•

```typescript
async function callAIWithRetry(
  apiCall: () => Promise<string>,
  maxRetries = 3,
  delayMs = 1000
): Promise<string> {
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      return await apiCall()
    } catch (error) {
      if (attempt === maxRetries) throw error
      
      // æŒ‡æ•°é€€é¿
      const delay = delayMs * Math.pow(2, attempt - 1)
      console.log(`é‡è¯• ${attempt}/${maxRetries}ï¼Œå»¶è¿Ÿ ${delay}ms`)
      await new Promise(resolve => setTimeout(resolve, delay))
    }
  }
}
```

### 2. æˆæœ¬ä¼˜åŒ–

```typescript
// ä¼°ç®—tokenæ•°é‡
function estimateTokens(text: string): number {
  // ç²—ç•¥ä¼°è®¡ï¼šè‹±æ–‡1tokenâ‰ˆ4å­—ç¬¦ï¼Œä¸­æ–‡1tokenâ‰ˆ1å­—ç¬¦
  const chineseCharCount = (text.match(/[\u4e00-\u9fa5]/g) || []).length
  const englishCharCount = text.replace(/[\u4e00-\u9fa5]/g, '').length
  return Math.ceil(chineseCharCount + englishCharCount / 4)
}

// è®¡ç®—æˆæœ¬
function calculateCost(
  inputTokens: number,
  outputTokens: number,
  modelId: string
): number {
  const prices: Record<string, { input: number; output: number }> = {
    'gpt-4-turbo': { input: 10, output: 30 },
    'gemini-2.5-pro': { input: 1.25, output: 2.5 },
    'gpt-4o-mini': { input: 0.15, output: 0.60 }
  }
  
  const price = prices[modelId] || { input: 0, output: 0 }
  return (inputTokens * price.input + outputTokens * price.output) / 1_000_000
}
```

### 3. ç¼“å­˜ç­–ç•¥

```typescript
interface CacheEntry {
  query: string
  response: string
  timestamp: number
  modelId: string
}

class AIResponseCache {
  private cache: CacheEntry[] = []
  private maxSize = 100
  private ttlMs = 24 * 60 * 60 * 1000 // 24å°æ—¶
  
  async get(query: string, modelId: string): Promise<string | null> {
    const entry = this.cache.find(
      e => e.query === query && 
           e.modelId === modelId && 
           Date.now() - e.timestamp < this.ttlMs
    )
    return entry?.response || null
  }
  
  set(query: string, response: string, modelId: string) {
    this.cache.push({
      query,
      response,
      modelId,
      timestamp: Date.now()
    })
    
    // é™åˆ¶ç¼“å­˜å¤§å°
    if (this.cache.length > this.maxSize) {
      this.cache.shift()
    }
  }
}
```

### 4. æ—¥å¿—ä¸ç›‘æ§

```typescript
interface APICallMetrics {
  modelId: string
  inputTokens: number
  outputTokens: number
  durationMs: number
  cost: number
  timestamp: number
  success: boolean
  errorType?: string
}

class APIMetricsCollector {
  private metrics: APICallMetrics[] = []
  
  record(metric: APICallMetrics) {
    this.metrics.push(metric)
    this.logToAnalytics(metric)
  }
  
  getStats() {
    return {
      totalCalls: this.metrics.length,
      totalCost: this.metrics.reduce((sum, m) => sum + m.cost, 0),
      avgDurationMs: this.metrics.reduce((sum, m) => sum + m.durationMs, 0) / this.metrics.length,
      successRate: this.metrics.filter(m => m.success).length / this.metrics.length,
      costByModel: this.groupByCost()
    }
  }
  
  private groupByCost() {
    return Object.fromEntries(
      Object.entries(
        this.metrics.reduce((acc, m) => {
          acc[m.modelId] = (acc[m.modelId] || 0) + m.cost
          return acc
        }, {} as Record<string, number>)
      )
    )
  }
  
  private logToAnalytics(metric: APICallMetrics) {
    console.log(`[API Call] ${metric.modelId} - ${metric.durationMs}ms - $${metric.cost.toFixed(6)}`)
  }
}
```

---

## å®Œæ•´ä»£ç ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šç®€å•èŠå¤©åº”ç”¨

```typescript
// src/services/aiService.ts
export interface AIConfig {
  provider: 'openai' | 'gemini'
  apiKey: string
  baseUrl?: string
  modelId: string
}

export async function chat(
  message: string,
  config: AIConfig,
  history: Array<{ role: 'user' | 'assistant'; content: string }> = []
): Promise<string> {
  if (config.provider === 'openai') {
    return callOpenAIAPI(message, config, history)
  } else if (config.provider === 'gemini') {
    return callGeminiAPI(message, config, history)
  }
  throw new Error('ä¸æ”¯æŒçš„æä¾›å•†')
}

async function callOpenAIAPI(
  message: string,
  config: AIConfig,
  history: Array<{ role: 'user' | 'assistant'; content: string }>
): Promise<string> {
  const response = await fetch(`${config.baseUrl || 'https://api.openai.com/v1'}/chat/completions`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${config.apiKey}`
    },
    body: JSON.stringify({
      model: config.modelId,
      messages: [...history, { role: 'user' as const, content: message }],
      temperature: 0.7
    })
  })

  if (!response.ok) {
    const error = await response.json()
    throw new Error(error.error?.message || 'APIè°ƒç”¨å¤±è´¥')
  }

  const data = await response.json()
  return data.choices[0].message.content
}

async function callGeminiAPI(
  message: string,
  config: AIConfig,
  history: Array<{ role: 'user' | 'assistant'; content: string }>
): Promise<string> {
  // å¦‚æœä½¿ç”¨å®˜æ–¹SDK
  if (typeof window === 'undefined') {
    const { GoogleGenAI } = await import('@google/genai')
    const genAI = new GoogleGenAI({ apiKey: config.apiKey })
    const model = genAI.models.getModel(config.modelId)
    
    const response = await model.generateContent({
      contents: message
    })
    
    return response.text
  }
  
  // æµè§ˆå™¨ç¯å¢ƒï¼Œä½¿ç”¨REST API
  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/${config.modelId}:generateContent?key=${config.apiKey}`,
    {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        contents: [{ parts: [{ text: message }] }]
      })
    }
  )

  const data = await response.json()
  return data.candidates[0].content.parts[0].text
}
```

### ç¤ºä¾‹2ï¼šå¸¦æµå¼è¾“å‡ºçš„Reactç»„ä»¶

```typescript
// src/components/ChatBox.tsx
import { useState } from 'react'
import { streamAI } from '../services/streamService'

export function ChatBox() {
  const [input, setInput] = useState('')
  const [output, setOutput] = useState('')
  const [loading, setLoading] = useState(false)
  const [abortController, setAbortController] = useState<AbortController | null>(null)

  async function handleSend() {
    if (!input.trim()) return

    setLoading(true)
    setOutput('')
    const controller = new AbortController()
    setAbortController(controller)

    try {
      for await (const chunk of streamAI(input, {
        provider: 'openai',
        apiKey: process.env.REACT_APP_OPENAI_KEY || '',
        baseUrl: 'https://api.openai.com/v1',
        modelId: 'gpt-4o'
      }, controller.signal)) {
        setOutput(prev => prev + chunk)
      }
    } catch (error) {
      if (error instanceof Error && error.name !== 'AbortError') {
        setOutput(prev => prev + `\n\n[é”™è¯¯] ${error.message}`)
      }
    } finally {
      setLoading(false)
      setAbortController(null)
    }
  }

  function handleStop() {
    abortController?.abort()
  }

  return (
    <div className="space-y-4">
      <textarea
        value={input}
        onChange={e => setInput(e.target.value)}
        placeholder="è¾“å…¥ä½ çš„é—®é¢˜..."
        className="w-full p-3 border rounded"
      />
      <div className="flex gap-2">
        <button
          onClick={handleSend}
          disabled={loading}
          className="px-4 py-2 bg-blue-500 text-white rounded disabled:opacity-50"
        >
          å‘é€
        </button>
        {loading && (
          <button
            onClick={handleStop}
            className="px-4 py-2 bg-red-500 text-white rounded"
          >
            åœæ­¢
          </button>
        )}
      </div>
      {loading && <div className="text-gray-500">å¤„ç†ä¸­...</div>}
      <div className="p-4 bg-gray-100 rounded min-h-24 whitespace-pre-wrap">
        {output || '(ç­‰å¾…å“åº”)'}
      </div>
    </div>
  )
}
```

---

## å­¦ä¹ è·¯å¾„å»ºè®®

### Week 1: åŸºç¡€æ¦‚å¿µ
- [ ] ç†è§£HTTP APIè°ƒç”¨åŸºç¡€
- [ ] å­¦ä¹ OpenAIå…¼å®¹APIæ ¼å¼
- [ ] å®ç°ç®€å•çš„éæµå¼APIè°ƒç”¨

### Week 2: è¿›é˜¶é›†æˆ
- [ ] å®ç°æµå¼è¾“å‡º
- [ ] å­¦ä¹ é€‚é…å™¨æ¨¡å¼ç»Ÿä¸€ä¸åŒAPI
- [ ] å¤„ç†é”™è¯¯å’Œé‡è¯•

### Week 3: ç”Ÿäº§å°±ç»ª
- [ ] æ·»åŠ ç¼“å­˜æœºåˆ¶
- [ ] å®ç°æˆæœ¬ç›‘æ§
- [ ] éƒ¨ç½²åˆ°Vercel

### Week 4: ä¼˜åŒ–ä¸å®è·µ
- [ ] å¤šæ¨¡å‹é€‰æ‹©é€»è¾‘
- [ ] æ€§èƒ½ä¼˜åŒ–ï¼ˆä»¤ç‰Œè®¡ç®—ã€æˆæœ¬ä¼°ç®—ï¼‰
- [ ] æ„å»ºä½ çš„ç¬¬ä¸€ä¸ªAIåº”ç”¨

---

## å¿«é€Ÿå‚è€ƒ

### è·å–APIå¯†é’¥
- **OpenAI**: https://platform.openai.com/api-keys
- **Google Gemini**: https://aistudio.google.com/app/apikey
- **Anthropic Claude**: https://console.anthropic.com/
- **Ollama**: æœ¬åœ°éƒ¨ç½²ï¼Œæ— éœ€å¯†é’¥

### å¸¸ç”¨å‘½ä»¤
```bash
# å¯åŠ¨æœ¬åœ°Ollama
ollama serve

# å¯åŠ¨LM Studio APIæœåŠ¡å™¨
# (é€šè¿‡LM Studio GUI)

# æµ‹è¯•APIè¿æ¥
curl -X POST http://localhost:11434/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3",
    "messages": [{"role": "user", "content": "Hello"}],
    "stream": false
  }'
```

### è´¹ç”¨ä¼°ç®—å·¥å…·

```typescript
// å¿«é€Ÿè®¡ç®—æˆæœ¬
const costs = {
  'gpt-4-turbo': { in: 10, out: 30 },      // æ¯ç™¾ä¸‡tokens
  'gpt-4o': { in: 5, out: 15 },
  'gpt-4o-mini': { in: 0.15, out: 0.60 },
  'gemini-2.5-pro': { in: 1.25, out: 2.5 },
  'gemini-2.5-flash': { in: 0.075, out: 0.30 }
}

function estimateCost(input: number, output: number, model: string) {
  const rate = costs[model] || { in: 1, out: 1 }
  return ((input * rate.in + output * rate.out) / 1_000_000).toFixed(6)
}
```

---

## ç›¸å…³èµ„æº

- ğŸ“– [OpenAI APIæ–‡æ¡£](https://platform.openai.com/docs)
- ğŸ“– [Google Gemini APIæ–‡æ¡£](https://ai.google.dev/docs)
- ğŸ“– [Anthropic Claudeæ–‡æ¡£](https://docs.anthropic.com/)
- ğŸ”§ [Ollamaå®˜ç½‘](https://ollama.ai/)
- ğŸ”§ [LM Studio](https://lmstudio.ai/)
- ğŸ“š [åŸé¡¹ç›®ä»£ç ](../dual-ai-chat/) - å®Œæ•´çš„ç”Ÿäº§çº§å®ç°

---

**å‡†å¤‡å¥½å¼€å§‹AIä¹‹æ—…äº†å—ï¼Ÿä¸‹ä¸€æ­¥ï¼šé€‰æ‹©ä¸€ä¸ªæä¾›å•†ï¼Œè·å–APIå¯†é’¥ï¼Œæ„å»ºä½ çš„ç¬¬ä¸€ä¸ªAIåº”ç”¨ï¼**
